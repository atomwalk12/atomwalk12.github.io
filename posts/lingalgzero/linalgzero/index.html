<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LinAlgZero: A linear algebra dataset for reasoning | Atom</title><meta name=keywords content="Linear Algebra,Reasoning,Dataset"><meta name=description content="A toolkit for generating a linear algebra dataset and training models for linear algebra reasoning."><meta name=author content="Razvan Florian Vasile"><link rel=canonical href=https://atomwalk12.github.io/posts/lingalgzero/linalgzero/><link crossorigin=anonymous href=/assets/css/stylesheet.3d56f230ca2f85049fc2a9ed80d77e2e8584cdb08dc1bcc9814720dd1b68a9a6.css integrity="sha256-PVbyMMovhQSfwqntgNd+LoWEzbCNwbzJgUcg3RtoqaY=" rel="preload stylesheet" as=style><link rel=icon href=https://atomwalk12.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://atomwalk12.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://atomwalk12.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://atomwalk12.github.io/apple-touch-icon.png><link rel=mask-icon href=https://atomwalk12.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://atomwalk12.github.io/posts/lingalgzero/linalgzero/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/github-dark.min.css><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css integrity=sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js integrity=sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script><meta property="og:url" content="https://atomwalk12.github.io/posts/lingalgzero/linalgzero/"><meta property="og:site_name" content="Atom"><meta property="og:title" content="LinAlgZero: A linear algebra dataset for reasoning"><meta property="og:description" content="A toolkit for generating a linear algebra dataset and training models for linear algebra reasoning."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-10T10:00:00+00:00"><meta property="article:modified_time" content="2025-06-10T10:00:00+00:00"><meta property="article:tag" content="Linear Algebra"><meta property="article:tag" content="Reasoning"><meta property="article:tag" content="Dataset"><meta property="og:image" content="https://atomwalk12.github.io/images/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://atomwalk12.github.io/images/papermod-cover.png"><meta name=twitter:title content="LinAlgZero: A linear algebra dataset for reasoning"><meta name=twitter:description content="A toolkit for generating a linear algebra dataset and training models for linear algebra reasoning."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://atomwalk12.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Linear Algebra Zero (LingAlgZero)","item":"https://atomwalk12.github.io/posts/lingalgzero/"},{"@type":"ListItem","position":3,"name":"LinAlgZero: A linear algebra dataset for reasoning","item":"https://atomwalk12.github.io/posts/lingalgzero/linalgzero/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LinAlgZero: A linear algebra dataset for reasoning","name":"LinAlgZero: A linear algebra dataset for reasoning","description":"A toolkit for generating a linear algebra dataset and training models for linear algebra reasoning.","keywords":["Linear Algebra","Reasoning","Dataset"],"articleBody":"Main Workflow Figure ?? represents an illustration of the development process, organised around a two-stage workflow for training a reasoning language model.\nFigure 1: The development process includes five main phases: (1) SFT Dataset Generation (M.1-M.3), (2) SFT Training (M.4), (3) RLVR Data Curation (M.5), (4) RLVR Training (M.6), and (5) Deployment (M.7-M.9). M.1 Initial Raw Data Collection: Creating template schemes for various problem types (e.g. Matrix Multiplication, Linear Equations, etc.). Defines hyperparameters to ensure varying difficulty levels concerning the format and output constraints. M.2 Synthetic Data Generation: Generating raw problems via synthetic data generation. M.3 Data Distillation: Preparing the dataset for the Supervised Fine-Tuning (SFT) stage. S.1 Chain-of-Thought (CoT) Generation: Generating step-by-step reasoning traces for collected questions using a powerful “teacher” model (i.e. DeepSeek-R1). S.2 Verification: Validating the correctness of answers and CoTs through methods like Math Verify or LLM judges. S.3 Difficulty-Based Filtering: Selecting problems based on their difficulty level. Samples with moderate pass rates are retained, while those that are too easy or too hard are filtered out. Filtering may also involve learning impact measurement or off-the-shelf LLM judges. S.4 Rigorous Data Cleaning: Performing deduplication (via embedding similarity or n-gram), rejection sampling, and decontamination to ensure high-quality and unbiased data. M.4 Supervised Fine-tuning (SFT) Training: To enable the base model to imitate high-quality reasoning traces from stronger models. This phase serves as a baseline for stable training. The entire reasoning dataset is used. M.5 Reinforcement Learning from Verifiable Rewards (RLVR) Data Curation: This is a distinct and critical phase that involves preparing high-quality, verifiable datasets specifically for RL training. This includes: Construction of Verified Questions and Answers:: Gathering datasets for RL, often focused on math problems that can be objectively verified via automated tools. Difficulty-Based Filtering: RL datasets are often designed to include samples where models are likely to make mistakes (i.e., moderate pass rates), as samples that are too easy or too hard offer limited learning opportunities and are often filtered out. This differs from SFT’s general emphasis on difficulty and diversity to ensure data richness. Detailed Pre-processing (Verification, Filtering, Cleaning): Rigorous steps similar to SFT data preparation, but tailored for RL (e.g., filtering for samples with moderate pass rates, removing unverifiable formats, and ensuring decontamination). M.6 Reinforcement Learning from Verifiable Rewards (RLVR) Training: The core RL phase that further enhances reasoning capabilities. This process includes: V.1 RL Algorithm Implementation: Utilizing and adapting algorithms such as PPO, GRPO, or their variants. V.2 Reward System Design: Defining rule-based reward functions (e.g., accuracy, format, length) that guide the model’s learning process. V.3 Sampling Strategies and Curriculum Learning: Employing techniques like dynamic sampling, epoch-level history resampling, or progressively increasing context/response length to optimize training efficiency and stability. V.4 Reasoning Model: As a result of the actor-critic optimisations, a reasoning model is obtained. M.7 Model Evaluation: Attentive assessment of the Reasoning Language Model’s (RLM) performance on relevant benchmarks (e.g., AIME, MATH500). M.8 Further Refinement: The model can be quantized to facilitate easier adoption. M.9 Deployment and Release: Making the trained reasoning language model and dataset available on HuggingFace. Experimental setting This library utilises Qwen3-1.7B as the base model. However, other models that have been considered include: Qwen3-4B, Qwen3-0.6B, phi4-4B-instruct, gemma-3-1B and gemma-3-4B.\nPost-training Training methods. This library opts to analyse the model’s emergent reasoning abilities in two ways. One method involves Reinforcement Learning with Verifiable Rewards (RLVR), while the other is centred around a distillation process involving existing strong reasoning models. Figure ?? provides an illustration of the two processes. Qwen3 (Yang et al., 2025 Yang An, et al. (2025). Qwen3 Technical Report. https://doi.org/10.48550/arXiv.2505.09388 ) suggests that training small models only using distillation yields better results than using RLVR, however in this library both methods are explored.\nFigure 2: Overview of the experimental setting in Qwen3: (1) RLVR for large models (\u003e32B parameters), (2) Distillation for small models (\u003c14B parameters). Our library demonstrates both approaches on a common base model: Qwen3-1.7B. (Image Source: Yang et al., 2025 Yang An, et al. (2025). Qwen3 Technical Report. https://doi.org/10.48550/arXiv.2505.09388 ) Experiment 1: RLVR Reinforcement Learning with Verifiable Rewards (RLVR) is a strategy for training LLMs where the reward signal is directly derived from the correctness of the model’s final answer on verifiable tasks. The technique was extensively used and pioneered by DeepSeek with their DeepSeek-R1-Zero paper (DeepSeek-AI et al., 2025 DeepSeek-AI et al. (2025). DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. https://doi.org/10.48550/arXiv.2501.12948 ). Qwen3 (Yang et al., 2025 Yang An, et al. (2025). Qwen3 Technical Report. https://doi.org/10.48550/arXiv.2505.09388 ) subsequently refines the process to include 3 key phases: cold-start SFT, RLVR and tool use.\nStep 1: Cold-Start SFT. SFT generally teaches the model to follow a desired prompt format, effectively facilitating convergence in the subsequent stages. This is necessary as pre-trained models are generally trained for text-completion, making their adherence to question-answering prompts difficult.\nQwen3 (Yang et al., 2025 Yang An, et al. (2025). Qwen3 Technical Report. https://doi.org/10.48550/arXiv.2505.09388 ) utilises a challenging reasoning dataset exclusively for this phase, while Qwen2.5 (Qwen et al., 2025 Qwen et al. (2025). Qwen2.5 Technical Report. https://doi.org/10.48550/arXiv.2412.15115 ) opts for a combination of both reasoning and non-reasoning traces of varying difficulty.\nStep 2: RLVR Training. The second phase (RLVR) is a critical post-training stage, designed to improve a model’s reasoning abilities. This ensures that through a step-by-step thinking process the solutions become verifiable.\nStep 3: Tool Use. Lastly, we anticipate a last phase designed to teach the model to use tools. This is a generalisation of the RLVR training process, where the model is trained to use tools to solve problems.\nExperiment 2: Distillation Qwen3 (Yang et al., 2025 Yang An, et al. (2025). Qwen3 Technical Report. https://doi.org/10.48550/arXiv.2505.09388 ) suggests that the small models ($\\leq$ 14B parameters) perform better when trained using distillation instead of the full-fledged RLVR training. This distillation process is composed of two phases, namely off-policy and on-policy distillation.\nThis section introduces the key concepts around the knowledge-distillation process, first proposed by (Hinton et al., 2015 Hinton Geoffrey, et al. (2015). Distilling the Knowledge in a Neural Network. https://doi.org/10.48550/arXiv.1503.02531 ). Also, an analysis of the theory involved is available in this post.\nOff-policy Distillation. Aims to to use the outputs of a teacher (i.e. Qwen3-32B) model to demonstrate basic reasoning skills to a smaller model (i.e. Qwen3-1.7B-Base). This phase is less memory intensive than the on-policy phase as the responses of the teacher can be generated in advance. The smaller model is fine-tuned on the teacher’s responses, which lays as a solid foundation for the on-policy phase.\nOn-policy Distillation. During this phase, both the student and the teacher generate their own responses to queries sampled from the reasoning dataset. The student model is then fine-tuned by aligning its logits with those of the teacher model by minimising KL-divergence. This allows the student to expand its exploration space and enhance its reasoning abilities, while requiring a substantially lesser amount of GPU hours (1/10th) compared to utilising solely RLVR.\nImplementation Surveys. There exist various surveys on the topic of knowledge distillation (Xu et al., 2024 Xu Xiaohan, et al. (2024). A Survey on Knowledge Distillation of Large Language Models. https://doi.org/10.48550/arXiv.2402.13116 ; Gou et al., 2021 Gou Jianping, et al. (2021). Knowledge Distillation: A Survey. International Journal of Computer Vision, 129(6). 1789–1819. https://doi.org/10.1007/s11263-021-01453-z ), with various studies effectively demonstrating correct implementations of the distillation process. Below are compiled insights on the distillation process.\nTraining Heuristics. FastFormers (Kim \u0026 Awadalla, 2020 Kim Young Jin \u0026 Awadalla Hany Hassan. (2020). FastFormers: Highly Efficient Transformer Models for Natural Language Understanding. https://doi.org/10.48550/arXiv.2010.13382 ) provides great insights into the mechanics of the distillation process, namely what works and what doesn’t.\nNOTE: The FastFormers paper suggests differentiating between task-agnostic and task-specific fine-tuning. The later involves fine-tuning the teacher on the dataset prior to initiating the distillation process, which may lead to better accuracy. See Figure 1 of the paper.\nMoreover, it suggests that the distillation process is only effective as long as the tokenizers of each model produce the same output given similar inputs. The study postulates that different tokenizer embedding spaces result in different output embedding spaces, and this leads to knowledge transfer being not as effective. So, a similar check is necessary:\nfrom transformers import AutoTokenizer # Initialise tokenizers teacher = AutoTokenizer.from_pretrained(TEACHER_ID) student = AutoTokenizer.from_pretrained(STUDENT_ID) # Input sample = \"Some sample text.\" # Assert results assert teacher(sample) == student(sample), \"Teacher/Student tokenizer mismatch\" Dataset generation Dataset generation: The generation of the dataset is composed of a two-phase filtering process:\nQuery (Complexity) Filtering: the purpose is to ensure that only complex problems are included in the dataset. A powerful model (i.e. Qwen2.5-72B-Instruct) tries to solve the problem without chain-of-thought reasoning. If the answer is correct, the problem is considered simple and is discarded. Otherwise, if the attempt fails, the problem is included in the dataset.\nResponse Filtering: This process includes a verification step, where we ensure that only correct answers are included in the final long-CoT dataset. Because of this, answers that are incorrect or contain substantial repetition are discarded.\n","wordCount":"1468","inLanguage":"en","image":"https://atomwalk12.github.io/images/papermod-cover.png","datePublished":"2025-06-10T10:00:00Z","dateModified":"2025-06-10T10:00:00Z","author":{"@type":"Person","name":"Razvan Florian Vasile"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://atomwalk12.github.io/posts/lingalgzero/linalgzero/"},"publisher":{"@type":"Organization","name":"Atom","logo":{"@type":"ImageObject","url":"https://atomwalk12.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://atomwalk12.github.io/ accesskey=h title="Atom (Alt + H)">Atom</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div>|</div><ul id=menu><li><a href=https://atomwalk12.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://atomwalk12.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://atomwalk12.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://atomwalk12.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://atomwalk12.github.io/resources/ title=Resources><span>Resources</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://atomwalk12.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://atomwalk12.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://atomwalk12.github.io/posts/lingalgzero/>Linear Algebra Zero (LingAlgZero)</a></div><h1 class="post-title entry-hint-parent">LinAlgZero: A linear algebra dataset for reasoning</h1><div class=post-meta><span title='2025-06-10 10:00:00 +0000 UTC'>June 10, 2025</span>&nbsp;|&nbsp;7 min&nbsp;|&nbsp;Razvan Florian Vasile</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span>
<button id=toc-theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme" class=toc-theme-toggle>
<svg id="moon-toc" width="18" height="15" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun-toc" width="18" height="15" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></summary><div class=inner><ul><li><a href=#main-workflow aria-label="Main Workflow">Main Workflow</a><ul><li><a href=#experimental-setting aria-label="Experimental setting">Experimental setting</a></li></ul></li><li><a href=#post-training aria-label=Post-training>Post-training</a><ul><li><a href=#experiment-1-rlvr aria-label="Experiment 1: RLVR">Experiment 1: RLVR</a></li><li><a href=#experiment-2-distillation aria-label="Experiment 2: Distillation">Experiment 2: Distillation</a><ul><li><a href=#implementation aria-label=Implementation>Implementation</a></li></ul></li></ul></li><li><a href=#dataset-generation aria-label="Dataset generation">Dataset generation</a></li></ul></div></details></div></aside><script>let activeElement,elements;const tocLinkMap=new Map;let scrollTicking=!1;function onScroll(){if(scrollTicking)return;scrollTicking=!0,window.requestAnimationFrame(()=>{if((!elements||!elements.length)&&(elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),!elements.length)){scrollTicking=!1;return}activeElement=Array.from(elements).find(e=>{const t=getOffsetTop(e)-window.pageYOffset;return t>-10&&t<window.innerHeight/2})||activeElement,tocLinkMap.forEach((e,t)=>{e.classList.toggle("active",t===activeElement)}),scrollTicking=!1})}window.addEventListener("DOMContentLoaded",()=>{elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),elements.forEach(e=>{const n=encodeURI(e.id),t=document.querySelector(`.inner ul li a[href="#${n}"]`);t&&tocLinkMap.set(e,t)}),activeElement=elements[0];const e=tocLinkMap.get(activeElement);e&&e.classList.add("active"),window.addEventListener("scroll",onScroll,{passive:!0}),onScroll()},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;const t=e.getBoundingClientRect();return t.top+window.pageYOffset}</script><script>function updateToggleVisibility(){window.pageYOffset<=10?document.body.classList.remove("scrolled"):document.body.classList.add("scrolled")}updateToggleVisibility(),window.addEventListener("scroll",updateToggleVisibility)</script><style>h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]{scroll-margin-top:20px}</style><div class=post-content><h1 id=main-workflow>Main Workflow<a hidden class=anchor aria-hidden=true href=#main-workflow>#</a></h1><p><a class=figref href=#fig-workflow>Figure <span data-fig-id=fig-workflow>??</span></a>
<script>document.addEventListener("DOMContentLoaded",()=>{const e={};document.querySelectorAll("figure[id]").forEach((t,n)=>e[t.id]=n+1),document.querySelectorAll("span[data-fig-id]").forEach(t=>{const n=e[t.dataset.figId];n&&(t.textContent=n)})})</script>represents an illustration of the development process, organised around a two-stage workflow for training a reasoning language model.</p><figure id=fig-workflow style=text-align:center><img class=zoomable src=../assets/workflow-with-semi-comments.png alt="The development process includes five main phases: (1) SFT Dataset Generation (M.1-M.3), (2) SFT Training (M.4), (3) RLVR Data Curation (M.5), (4) RLVR Training (M.6), and (5) Deployment (M.7-M.9)."><figcaption style="text-align:center;width:80%;margin:10px auto 35px;line-height:1.35;color:#888;font-size:15px"><p style=font-size:15px><strong>Figure&nbsp;1: </strong>The development process includes five main phases: (1) SFT Dataset Generation (M.1-M.3), (2) SFT Training (M.4), (3) RLVR Data Curation (M.5), (4) RLVR Training (M.6), and (5) Deployment (M.7-M.9).<strong></strong></p></figcaption></figure><ul><li><strong>M.1 Initial Raw Data Collection</strong>: Creating template schemes for various problem types (e.g. Matrix Multiplication, Linear Equations, etc.). Defines hyperparameters to ensure varying difficulty levels concerning the format and output constraints.</li><li><strong>M.2 Synthetic Data Generation</strong>: Generating raw problems via synthetic data generation.</li><li><strong>M.3 Data Distillation</strong>: Preparing the dataset for the Supervised Fine-Tuning (SFT) stage. <span id=sec-data-distillation></span><ul><li><strong>S.1 Chain-of-Thought (CoT) Generation</strong>: Generating step-by-step reasoning traces for collected questions using a powerful &ldquo;teacher&rdquo; model (i.e. DeepSeek-R1).</li><li><strong>S.2 Verification</strong>: Validating the correctness of answers and CoTs through methods like Math Verify or LLM judges.</li><li><strong>S.3 Difficulty-Based Filtering</strong>: Selecting problems based on their difficulty level. Samples with moderate pass rates are retained, while those that are too easy or too hard are filtered out. Filtering may also involve learning impact measurement or off-the-shelf LLM judges.</li><li><strong>S.4 Rigorous Data Cleaning</strong>: Performing deduplication (via embedding similarity or n-gram), rejection sampling, and decontamination to ensure high-quality and unbiased data.</li></ul></li><li><span id=base-models-sec></span> <strong>M.4 Supervised Fine-tuning (SFT) Training</strong>: To enable the base model to imitate high-quality reasoning traces from stronger models. This phase serves as a baseline for stable training. The entire reasoning dataset is used.</li><li><strong>M.5 Reinforcement Learning from Verifiable Rewards (RLVR) Data Curation</strong>: This is a distinct and critical phase that involves preparing high-quality, verifiable datasets specifically for RL training. This includes:<ul><li><strong>Construction of Verified Questions and Answers</strong>:: Gathering datasets for RL, often focused on math problems that can be objectively verified via automated tools.</li><li><strong>Difficulty-Based Filtering</strong>: RL datasets are often designed to include samples where models are likely to make mistakes (i.e., moderate pass rates), as samples that are too easy or too hard offer limited learning opportunities and are often filtered out. This differs from SFT&rsquo;s general emphasis on difficulty and diversity to ensure data richness.</li><li><strong>Detailed Pre-processing (Verification, Filtering, Cleaning)</strong>: Rigorous steps similar to SFT data preparation, but tailored for RL (e.g., filtering for samples with moderate pass rates, removing unverifiable formats, and ensuring decontamination).</li></ul></li><li><span id=sec-rlvr-training></span> <strong>M.6 Reinforcement Learning from Verifiable Rewards (RLVR) Training</strong>: The core RL phase that further enhances reasoning capabilities. This process includes:<ul><li><strong>V.1 RL Algorithm Implementation</strong>: Utilizing and adapting algorithms such as PPO, GRPO, or their variants.</li><li><strong>V.2 Reward System Design</strong>: Defining rule-based reward functions (e.g., accuracy, format, length) that guide the model&rsquo;s learning process.</li><li><strong>V.3 Sampling Strategies and Curriculum Learning</strong>: Employing techniques like dynamic sampling, epoch-level history resampling, or progressively increasing context/response length to optimize training efficiency and stability.</li><li><strong>V.4 Reasoning Model</strong>: As a result of the actor-critic optimisations, a reasoning model is obtained.</li></ul></li><li><strong>M.7 Model Evaluation</strong>: Attentive assessment of the Reasoning Language Model&rsquo;s (RLM) performance on relevant benchmarks (e.g., AIME, MATH500).</li><li><strong>M.8 Further Refinement</strong>: The model can be quantized to facilitate easier adoption.</li><li><strong>M.9 Deployment and Release</strong>: Making the trained reasoning language model and dataset available on HuggingFace.</li></ul><h2 id=experimental-setting>Experimental setting<a hidden class=anchor aria-hidden=true href=#experimental-setting>#</a></h2><p>This library utilises <a href=https://huggingface.co/Qwen/Qwen3-1.7B>Qwen3-1.7B</a> as the base model. However, other models that have been considered include: <a href=https://huggingface.co/Qwen/Qwen3-4B>Qwen3-4B</a>, <a href=https://huggingface.co/Qwen/Qwen3-0.6B>Qwen3-0.6B</a>, <a href=https://huggingface.co/microsoft/Phi-4-mini-instruct>phi4-4B-instruct</a>, <a href=https://huggingface.co/google/gemma-3-1b-pt>gemma-3-1B</a> and <a href=https://huggingface.co/google/gemma-3-4b-pt>gemma-3-4B</a>.</p><h1 id=post-training>Post-training<a hidden class=anchor aria-hidden=true href=#post-training>#</a></h1><p><strong>Training methods.</strong> This library opts to analyse the model&rsquo;s emergent reasoning abilities in two ways. One method involves Reinforcement Learning with Verifiable Rewards (RLVR), while the other is centred around a distillation process involving existing strong reasoning models.
<a class=figref href=#fig-training-methods>Figure <span data-fig-id=fig-training-methods>??</span></a>
<script>document.addEventListener("DOMContentLoaded",()=>{const e={};document.querySelectorAll("figure[id]").forEach((t,n)=>e[t.id]=n+1),document.querySelectorAll("span[data-fig-id]").forEach(t=>{const n=e[t.dataset.figId];n&&(t.textContent=n)})})</script>provides an illustration of the two processes. Qwen3
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group><a href=#qwen3><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="An"><span itemprop=familyName>Yang</span></span> 
 et al., <span itemprop=datePublished>2025</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yang</span> 
<meta itemprop=givenName content="An">An,</span> et al.
 
(<span itemprop=datePublished>2025</span>).
 <span itemprop=name>Qwen3 Technical Report</span>.
<a href=https://doi.org/10.48550/arXiv.2505.09388 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2505.09388</a></span>
</span></span>)</span> suggests that training small models only using distillation yields better results than using RLVR, however in this library both methods are explored.</p><figure id=fig-training-methods style=text-align:center><img class=zoomable src=../assets/qwen3-stages.png alt="Overview of the experimental setting in Qwen3: (1) RLVR for large models (>32B parameters), (2) Distillation for small models (<14B parameters). Our library demonstrates both approaches on a common base model: Qwen3-1.7B." style=text-align:center;width:90%;margin-left:auto;margin-right:auto><figcaption style="text-align:center;width:80%;margin:10px auto 35px;line-height:1.35;color:#888;font-size:15px"><p style=font-size:15px><strong>Figure&nbsp;2: </strong>Overview of the experimental setting in Qwen3: (1) RLVR for large models (>32B parameters), (2) Distillation for small models (&lt;14B parameters). Our library demonstrates both approaches on a common base model: Qwen3-1.7B.<strong> </strong>(Image Source: <span><span><span class=hugo-cite-intext itemprop=citation><span class=hugo-cite-group><a href=#qwen3><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="An"><span itemprop=familyName>Yang</span></span> 
 et al., <span itemprop=datePublished>2025</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yang</span> 
<meta itemprop=givenName content="An">An,</span> et al.
 
(<span itemprop=datePublished>2025</span>).
 <span itemprop=name>Qwen3 Technical Report</span>.
<a href=https://doi.org/10.48550/arXiv.2505.09388 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2505.09388</a></span>
</span></span></span></span>)</span></p></figcaption></figure><h2 id=experiment-1-rlvr>Experiment 1: RLVR<a hidden class=anchor aria-hidden=true href=#experiment-1-rlvr>#</a></h2><p>Reinforcement Learning with Verifiable Rewards (RLVR) is a strategy for training LLMs where the reward signal is directly derived from the correctness of the model&rsquo;s final answer on verifiable tasks. The technique was extensively used and pioneered by DeepSeek with their DeepSeek-R1-Zero paper
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group><a href=#deepseek-r1><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>DeepSeek-AI</span></span> 
 et al., <span itemprop=datePublished>2025</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>DeepSeek-AI</span></span> et al.
 
(<span itemprop=datePublished>2025</span>).
 <span itemprop=name>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</span>.
<a href=https://doi.org/10.48550/arXiv.2501.12948 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2501.12948</a></span>
</span></span>)</span>.
Qwen3
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group><a href=#qwen3><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="An"><span itemprop=familyName>Yang</span></span> 
 et al., <span itemprop=datePublished>2025</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yang</span> 
<meta itemprop=givenName content="An">An,</span> et al.
 
(<span itemprop=datePublished>2025</span>).
 <span itemprop=name>Qwen3 Technical Report</span>.
<a href=https://doi.org/10.48550/arXiv.2505.09388 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2505.09388</a></span>
</span></span>)</span> subsequently refines the process to include 3 key phases: cold-start SFT, RLVR and tool use.</p><p><strong>Step 1: Cold-Start SFT</strong>. SFT generally teaches the model to follow a desired prompt format, effectively facilitating convergence in the subsequent stages. This is necessary as pre-trained models are generally trained for text-completion, making their adherence to question-answering prompts difficult.</p><p>Qwen3
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group><a href=#qwen3><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="An"><span itemprop=familyName>Yang</span></span> 
 et al., <span itemprop=datePublished>2025</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yang</span> 
<meta itemprop=givenName content="An">An,</span> et al.
 
(<span itemprop=datePublished>2025</span>).
 <span itemprop=name>Qwen3 Technical Report</span>.
<a href=https://doi.org/10.48550/arXiv.2505.09388 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2505.09388</a></span>
</span></span>)</span> utilises a challenging reasoning dataset exclusively for this phase, while Qwen2.5
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group><a href=#qwen2.5><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Qwen</span></span> 
 et al., <span itemprop=datePublished>2025</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Qwen</span></span> et al.
 
(<span itemprop=datePublished>2025</span>).
 <span itemprop=name>Qwen2.5 Technical Report</span>.
<a href=https://doi.org/10.48550/arXiv.2412.15115 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2412.15115</a></span>
</span></span>)</span> opts for a combination of both reasoning and non-reasoning traces of varying difficulty.</p><p><span id=sec-detailed-rlvr-training></span><strong>Step 2: RLVR Training.</strong> The second phase (RLVR) is a critical post-training stage, designed to improve a model&rsquo;s reasoning abilities. This ensures that through a step-by-step thinking process the solutions become verifiable.</p><p><span id=sec-detailed-tool-use></span><strong>Step 3: Tool Use.</strong> Lastly, we anticipate a last phase designed to teach the model to use tools. This is a generalisation of the RLVR training process, where the model is trained to use tools to solve problems.</p><h2 id=experiment-2-distillation>Experiment 2: Distillation<a hidden class=anchor aria-hidden=true href=#experiment-2-distillation>#</a></h2><p>Qwen3
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group><a href=#qwen3><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="An"><span itemprop=familyName>Yang</span></span> 
 et al., <span itemprop=datePublished>2025</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yang</span> 
<meta itemprop=givenName content="An">An,</span> et al.
 
(<span itemprop=datePublished>2025</span>).
 <span itemprop=name>Qwen3 Technical Report</span>.
<a href=https://doi.org/10.48550/arXiv.2505.09388 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2505.09388</a></span>
</span></span>)</span> suggests that the small models ($\leq$ 14B parameters) perform better when trained using distillation instead of the full-fledged RLVR training. This distillation process is composed of two phases, namely off-policy and on-policy distillation.</p><p>This section introduces the key concepts around the knowledge-distillation process, first proposed by
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group><a href=#distillation-hinton><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Geoffrey"><span itemprop=familyName>Hinton</span></span> 
 et al., <span itemprop=datePublished>2015</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Hinton</span> 
<meta itemprop=givenName content="Geoffrey">Geoffrey,</span> et al.
 
(<span itemprop=datePublished>2015</span>).
 <span itemprop=name>Distilling the Knowledge in a Neural Network</span>.
<a href=https://doi.org/10.48550/arXiv.1503.02531 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.1503.02531</a></span>
</span></span>)</span>. Also, an analysis of the theory involved is available in <a href=/posts/lingalgzero/theory/#distillation>this post</a>.</p><p><strong>Off-policy Distillation.</strong> Aims to to use the outputs of a teacher (i.e. Qwen3-32B) model to demonstrate basic reasoning skills to a smaller model (i.e. Qwen3-1.7B-Base). This phase is less memory intensive than the on-policy phase as the responses of the teacher can be generated in advance. The smaller model is fine-tuned on the teacher&rsquo;s responses, which lays as a solid foundation for the on-policy phase.</p><p><strong>On-policy Distillation.</strong> During this phase, both the student and the teacher generate their own responses to queries sampled from the reasoning dataset. The student model is then fine-tuned by aligning its logits with those of the teacher model by minimising KL-divergence. This allows the student to expand its exploration space and enhance its reasoning abilities, while requiring a substantially lesser amount of GPU hours (1/10th) compared to utilising solely RLVR.</p><h3 id=implementation>Implementation<a hidden class=anchor aria-hidden=true href=#implementation>#</a></h3><p><strong>Surveys.</strong> There exist various surveys on the topic of knowledge distillation
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group><a href=#knowledge-distillation-survey><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Xiaohan"><span itemprop=familyName>Xu</span></span> 
 et al., <span itemprop=datePublished>2024</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Xu</span> 
<meta itemprop=givenName content="Xiaohan">Xiaohan,</span> et al.
 
(<span itemprop=datePublished>2024</span>).
 <span itemprop=name>A Survey on Knowledge Distillation of Large Language Models</span>.
<a href=https://doi.org/10.48550/arXiv.2402.13116 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2402.13116</a></span>
</span></span>; <span class=hugo-cite-group><a href=#knowledge-distillation-survey-2><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jianping"><span itemprop=familyName>Gou</span></span> 
 et al., <span itemprop=datePublished>2021</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gou</span> 
<meta itemprop=givenName content="Jianping">Jianping,</span> et al.
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name>Knowledge Distillation: A Survey</span>.<i>
<span itemprop=about>International Journal of Computer Vision</span>, 129(6)</i>. <span itemprop=pagination>1789–1819</span>.
<a href=https://doi.org/10.1007/s11263-021-01453-z target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.1007/s11263-021-01453-z</a></span>
</span></span>)</span>, with various studies effectively demonstrating correct implementations of the distillation process. Below are compiled insights on the distillation process.</p><p><strong>Training Heuristics.</strong> FastFormers
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group><a href=#fastformers><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Young Jin"><span itemprop=familyName>Kim</span></span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Hany Hassan"><span itemprop=familyName>Awadalla</span></span>, <span itemprop=datePublished>2020</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kim</span> 
<meta itemprop=givenName content="Young Jin">Young Jin</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Awadalla</span> 
<meta itemprop=givenName content="Hany Hassan">Hany Hassan.</span>
 
(<span itemprop=datePublished>2020</span>).
 <span itemprop=name>FastFormers: Highly Efficient Transformer Models for Natural Language Understanding</span>.
<a href=https://doi.org/10.48550/arXiv.2010.13382 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2010.13382</a></span>
</span></span>)</span> provides great insights into the mechanics of the distillation process, namely what works and what doesn&rsquo;t.</p><blockquote><p><strong><em>NOTE:</em></strong>
The FastFormers paper suggests differentiating between task-agnostic and task-specific fine-tuning. The later involves fine-tuning the teacher on the dataset prior to initiating the distillation process, which may lead to better accuracy. See Figure 1 of the paper.</p></blockquote><p>Moreover, it suggests that the distillation process is only effective as long as the tokenizers of each model produce the same output given similar inputs. The study postulates that different tokenizer embedding spaces result in different output embedding spaces, and this leads to knowledge transfer being not as effective. So, a similar check is necessary:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python3 data-lang=python3><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialise tokenizers</span>
</span></span><span class=line><span class=cl><span class=n>teacher</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>TEACHER_ID</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>student</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>STUDENT_ID</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Input</span>
</span></span><span class=line><span class=cl><span class=n>sample</span> <span class=o>=</span> <span class=s2>&#34;Some sample text.&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Assert results</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>teacher</span><span class=p>(</span><span class=n>sample</span><span class=p>)</span> <span class=o>==</span> <span class=n>student</span><span class=p>(</span><span class=n>sample</span><span class=p>),</span> <span class=s2>&#34;Teacher/Student tokenizer mismatch&#34;</span>
</span></span></code></pre></div><h1 id=dataset-generation>Dataset generation<a hidden class=anchor aria-hidden=true href=#dataset-generation>#</a></h1><p><strong>Dataset generation</strong>: The generation of the dataset is composed of a two-phase filtering process:</p><ul><li><p><strong>Query (Complexity) Filtering</strong>: the purpose is to ensure that only complex problems are included in the dataset. A powerful model (i.e. Qwen2.5-72B-Instruct) tries to solve the problem without chain-of-thought reasoning. If the answer is correct, the problem is considered simple and is discarded. Otherwise, if the attempt fails, the problem is included in the dataset.</p></li><li><p><strong>Response Filtering</strong>: This process includes a verification step, where we ensure that only correct answers are included in the final long-CoT dataset. Because of this, answers that are incorrect or contain substantial repetition are discarded.</p></li></ul><h1>Citation</h1><div class=post-citation><p>If you found this post useful for your work, please consider citing it as:</p><blockquote>Razvan Florian Vasile. (June 2025). "LinAlgZero: A linear algebra dataset for reasoning". Atom Blog. Retrieved from <a href=https://atomwalk12.github.io/posts/lingalgzero/linalgzero/>https://atomwalk12.github.io/posts/lingalgzero/linalgzero/</a>.</blockquote><p>or</p><pre tabindex=0><code class="language-typescript citation-code">@misc{vasile2025linalgzero,
    title = "LinAlgZero: A linear algebra dataset for reasoning",
    author = "Razvan Florian Vasile",
    note = "Personal blog",
    year = "2025",
    month = "June",
    url = "https://atomwalk12.github.io/posts/lingalgzero/linalgzero/"
}
</code></pre><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelector(".citation-code");e&&typeof hljs!="undefined"&&hljs.highlightElement(e)})</script></div><h1 id=references>References</h1><section><ol class=hugo-cite-bibliography><li id=deepseek-r1>[1]
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>DeepSeek-AI</span></span> et al.
 
(<span itemprop=datePublished>2025</span>).
 <span itemprop=name>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</span>.
<a href=https://doi.org/10.48550/arXiv.2501.12948 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2501.12948</a></span></li><li id=distillation-hinton>[2]
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Hinton</span> 
<meta itemprop=givenName content="Geoffrey">Geoffrey,</span> et al.
 
(<span itemprop=datePublished>2015</span>).
 <span itemprop=name>Distilling the Knowledge in a Neural Network</span>.
<a href=https://doi.org/10.48550/arXiv.1503.02531 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.1503.02531</a></span></li><li id=fastformers>[3]
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kim</span> 
<meta itemprop=givenName content="Young Jin">Young Jin</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Awadalla</span> 
<meta itemprop=givenName content="Hany Hassan">Hany Hassan.</span>
 
(<span itemprop=datePublished>2020</span>).
 <span itemprop=name>FastFormers: Highly Efficient Transformer Models for Natural Language Understanding</span>.
<a href=https://doi.org/10.48550/arXiv.2010.13382 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2010.13382</a></span></li><li id=knowledge-distillation-survey>[4]
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Xu</span> 
<meta itemprop=givenName content="Xiaohan">Xiaohan,</span> et al.
 
(<span itemprop=datePublished>2024</span>).
 <span itemprop=name>A Survey on Knowledge Distillation of Large Language Models</span>.
<a href=https://doi.org/10.48550/arXiv.2402.13116 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2402.13116</a></span></li><li id=knowledge-distillation-survey-2>[5]
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gou</span> 
<meta itemprop=givenName content="Jianping">Jianping,</span> et al.
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name>Knowledge Distillation: A Survey</span>.<i>
<span itemprop=about>International Journal of Computer Vision</span>, 129(6)</i>. <span itemprop=pagination>1789–1819</span>.
<a href=https://doi.org/10.1007/s11263-021-01453-z target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.1007/s11263-021-01453-z</a></span></li><li id=qwen2.5>[6]
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Qwen</span></span> et al.
 
(<span itemprop=datePublished>2025</span>).
 <span itemprop=name>Qwen2.5 Technical Report</span>.
<a href=https://doi.org/10.48550/arXiv.2412.15115 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2412.15115</a></span></li><li id=qwen3>[7]
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yang</span> 
<meta itemprop=givenName content="An">An,</span> et al.
 
(<span itemprop=datePublished>2025</span>).
 <span itemprop=name>Qwen3 Technical Report</span>.
<a href=https://doi.org/10.48550/arXiv.2505.09388 target=_blank rel="noopener noreferrer" itemprop=identifier itemtype=https://schema.org/URL>https://doi.org/10.48550/arXiv.2505.09388</a></span></li></ol></section></div><div class=margin-notes><div class=margin-note id=margin-note-1 data-target=1><h4>Workflow</h4><p>An illustration of the key steps in the development process.</p></div><div class=margin-note id=margin-note-2 data-target=2><h4>Step 1: Data Distillation</h4><p>Preparation of the base dataset used for both SFT and RLVR training. The later involve an additional data curation step.</p></div><div class=margin-note id=margin-note-3 data-target=3><h4>Step 2: SFT Training</h4><p>Possible base models include llama4, phi4, gemma3, qwen3.</p></div><div class=margin-note id=margin-note-4 data-target=4><h4>Step 3: RLVR Training</h4><p>This is the core process which involves a complex training complex involving an actor-critic optimiser.</p></div><div class=margin-note id=margin-note-5 data-target=5><h4>To complete</h4><p>This section needs to be expanded on.</p></div><div class=margin-note id=margin-note-6 data-target=6><h4>To complete</h4><p>This section needs to be expanded on.</p></div></div><div class=mobile-note-overlay id=mobile-note-overlay></div><div class=mobile-note-popup id=mobile-note-popup><div id=mobile-note-content></div></div><script type=application/json id=notes-data>"[{\"anchor\":\"fig-workflow\",\"content\":\"An illustration of the key steps in the development process.\",\"title\":\"Workflow\"},{\"anchor\":\"sec-data-distillation\",\"content\":\"Preparation of the base dataset used for both SFT and RLVR training. The later involve an additional data curation step.\",\"title\":\"Step 1: Data Distillation\"},{\"anchor\":\"base-models-sec\",\"content\":\"Possible base models include llama4, phi4, gemma3, qwen3.\",\"title\":\"Step 2: SFT Training\"},{\"anchor\":\"sec-rlvr-training\",\"content\":\"This is the core process which involves a complex training complex involving an actor-critic optimiser.\",\"title\":\"Step 3: RLVR Training\"},{\"anchor\":\"sec-detailed-rlvr-training\",\"content\":\"This section needs to be expanded on.\",\"title\":\"To complete\"},{\"anchor\":\"sec-detailed-tool-use\",\"content\":\"This section needs to be expanded on.\",\"title\":\"To complete\"}]"</script><script>document.addEventListener("DOMContentLoaded",function(){const t=document.getElementById("notes-data");if(!t){console.log("No notes-data element found");return}document.body.classList.add("notes-enabled");let e;try{const n=t.textContent.trim();if(!n){console.error("Notes data element is empty");return}e=JSON.parse(n),typeof e=="string"&&(e=JSON.parse(e))}catch(e){console.error("Failed to parse notes data:",e,"Raw content:",t.textContent);return}if(!e||!Array.isArray(e)||e.length===0){console.error("Notes data is not a valid array:",e);return}const o=document.querySelectorAll(".margin-note"),n=window.innerWidth<=1200;n||(s(),window.addEventListener("resize",d(s,250))),i();function i(){e.forEach(function(e,t){const o=t+1;let s=null;if(e.anchor)s=document.getElementById(e.anchor);else if(e.selector)s=document.querySelector(e.selector);else{const e=document.querySelectorAll(".post-content h1, .post-content h2, .post-content h3, .post-content h4, .post-content h5, .post-content h6");e[t]&&(s=e[t])}if(s){s.classList.add("note-target"),s.setAttribute("data-note",o);const t=document.createElement("span");t.className="note-marker",t.textContent=o,t.setAttribute("data-note",o),t.onclick=function(){l(o,e)},s.appendChild(t),n||(s.addEventListener("mouseenter",function(){r(o)}),s.addEventListener("mouseleave",function(){c(o)}))}else console.error("Could not find target element for note",o)})}function s(){const e=document.querySelector(".post-single");if(!e)return;requestAnimationFrame(function(){o.forEach(function(t){const n=t.id.replace("margin-note-",""),s=document.querySelector('[data-note="'+n+'"]');if(s){let n=a(s,e);t.style.top=n+"px"}else console.log("Target element not found for note",n)})})}function a(e,t){let s=0,n=e;for(;n&&n!==t;)s+=n.offsetTop,n=n.offsetParent;return s}function r(e){const t=document.getElementById("margin-note-"+e),n=document.querySelector('[data-note="'+e+'"]');t&&(t.style.zIndex="20"),n&&n.classList.add("highlighted")}function c(e){const t=document.getElementById("margin-note-"+e),n=document.querySelector('[data-note="'+e+'"]');t&&(t.style.zIndex="10"),n&&n.classList.remove("highlighted")}function l(e,t){if(window.innerWidth<=1200){const e=document.getElementById("mobile-note-popup"),n=document.getElementById("mobile-note-overlay"),s=document.getElementById("mobile-note-content");s.innerHTML=(t.title?"<h4>"+t.title+"</h4>":"")+"<p>"+t.content+"</p>",e.classList.add("show"),n.classList.add("show")}}function d(e,t){let n;return function(){const o=arguments,i=function(){clearTimeout(n),e.apply(null,o)};clearTimeout(n),n=setTimeout(i,t)}}});function closeMobileNote(){document.getElementById("mobile-note-popup").classList.remove("show"),document.getElementById("mobile-note-overlay").classList.remove("show")}document.addEventListener("click",function(e){e.target.id==="mobile-note-overlay"&&closeMobileNote()})</script><footer class=post-footer><ul class=post-tags><li><a href=https://atomwalk12.github.io/tags/linear-algebra/>Linear Algebra</a></li><li><a href=https://atomwalk12.github.io/tags/reasoning/>Reasoning</a></li><li><a href=https://atomwalk12.github.io/tags/dataset/>Dataset</a></li></ul><nav class=paginav><a class=next href=https://atomwalk12.github.io/posts/lingalgzero/theory/><span class=title>Next »</span><br><span>LinAlgZero: Theory</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share LinAlgZero: A linear algebra dataset for reasoning on x" href="https://x.com/intent/tweet/?text=LinAlgZero%3a%20A%20linear%20algebra%20dataset%20for%20reasoning&amp;url=https%3a%2f%2fatomwalk12.github.io%2fposts%2flingalgzero%2flinalgzero%2f&amp;hashtags=LinearAlgebra%2cReasoning%2cDataset"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LinAlgZero: A linear algebra dataset for reasoning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fatomwalk12.github.io%2fposts%2flingalgzero%2flinalgzero%2f&amp;title=LinAlgZero%3a%20A%20linear%20algebra%20dataset%20for%20reasoning&amp;summary=LinAlgZero%3a%20A%20linear%20algebra%20dataset%20for%20reasoning&amp;source=https%3a%2f%2fatomwalk12.github.io%2fposts%2flingalgzero%2flinalgzero%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LinAlgZero: A linear algebra dataset for reasoning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fatomwalk12.github.io%2fposts%2flingalgzero%2flinalgzero%2f&title=LinAlgZero%3a%20A%20linear%20algebra%20dataset%20for%20reasoning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LinAlgZero: A linear algebra dataset for reasoning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fatomwalk12.github.io%2fposts%2flingalgzero%2flinalgzero%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LinAlgZero: A linear algebra dataset for reasoning on whatsapp" href="https://api.whatsapp.com/send?text=LinAlgZero%3a%20A%20linear%20algebra%20dataset%20for%20reasoning%20-%20https%3a%2f%2fatomwalk12.github.io%2fposts%2flingalgzero%2flinalgzero%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LinAlgZero: A linear algebra dataset for reasoning on telegram" href="https://telegram.me/share/url?text=LinAlgZero%3a%20A%20linear%20algebra%20dataset%20for%20reasoning&amp;url=https%3a%2f%2fatomwalk12.github.io%2fposts%2flingalgzero%2flinalgzero%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LinAlgZero: A linear algebra dataset for reasoning on ycombinator" href="https://news.ycombinator.com/submitlink?t=LinAlgZero%3a%20A%20linear%20algebra%20dataset%20for%20reasoning&u=https%3a%2f%2fatomwalk12.github.io%2fposts%2flingalgzero%2flinalgzero%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>© 2025 <a href=https://github.com/atomwalk12>Atom</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><link rel=stylesheet type=text/css href=/hugo-cite.css><style>img.zoomable{cursor:zoom-in}body.zoom-active .margin-notes,body.zoom-active .note-marker,body.zoom-active .mobile-note-popup,body.zoom-active .mobile-note-overlay{display:none !important}</style><script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){const e=mediumZoom(".zoomable",{margin:200,background:"rgba(0, 0, 0, 0.8)",scrollOffset:40});e.on("open",()=>{document.body.classList.add("zoom-active")}),e.on("closed",()=>{document.body.classList.remove("zoom-active")})})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("#theme-toggle, #toc-theme-toggle").forEach(e=>{e.addEventListener("click",function(e){this.id==="toc-theme-toggle"&&e.preventDefault(),document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>